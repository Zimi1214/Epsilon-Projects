{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1OASEGEogteW4AOfeHYXbBU-F3ThFfohB",
      "authorship_tag": "ABX9TyPgFqu3v2/4GyuPFootm8gC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zimi1214/Final-Project/blob/master/Minimized%20code%20for%20prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLkcXmfR1V93",
        "colab_type": "code",
        "outputId": "ccb2e679-da6a-49b8-df6a-566c6ea3cd06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import re\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "seed=4353"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5McOd-y3zZ7e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4brCN6doqopu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "d0d4f315-40b7-4cbc-c245-b2560d05fbc4"
      },
      "source": [
        "df=pd.read_csv('/content/drive/My Drive/Data Science/clean.csv')\n",
        "df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>donald trump send embarrass new year eve messa...</td>\n",
              "      <td>News</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>drunk brag trump staffer start russian collu i...</td>\n",
              "      <td>News</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>sheriff david clark becom internet joke threat...</td>\n",
              "      <td>News</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>trump obsess even obama name code websit imag ...</td>\n",
              "      <td>News</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>pope franci call donald trump christma speechp...</td>\n",
              "      <td>News</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44893</th>\n",
              "      <td>21412</td>\n",
              "      <td>fulli commit nato back new us approach afghani...</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44894</th>\n",
              "      <td>21413</td>\n",
              "      <td>lexisnexi withdrew two product chine marketlon...</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44895</th>\n",
              "      <td>21414</td>\n",
              "      <td>minsk cultur hub becom authoritiesminsk reuter...</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44896</th>\n",
              "      <td>21415</td>\n",
              "      <td>vatican upbeat possibl pope franci visit russi...</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44897</th>\n",
              "      <td>21416</td>\n",
              "      <td>indonesia buy billion worth russian jetsjakart...</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>44898 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  ... type\n",
              "0               0  ...    0\n",
              "1               1  ...    0\n",
              "2               2  ...    0\n",
              "3               3  ...    0\n",
              "4               4  ...    0\n",
              "...           ...  ...  ...\n",
              "44893       21412  ...    1\n",
              "44894       21413  ...    1\n",
              "44895       21414  ...    1\n",
              "44896       21415  ...    1\n",
              "44897       21416  ...    1\n",
              "\n",
              "[44898 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsT2ME8azuQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop(columns=['Unnamed: 0','subject'],inplace=True,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx2vZxk4Lv9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to retrieve processed words\n",
        "\n",
        "def final(X_data_full):\n",
        "    \n",
        "    # function for removing punctuations\n",
        "    def remove_punct(X_data_func):\n",
        "        string1 = X_data_func.lower()\n",
        "        translation_table = dict.fromkeys(map(ord, string.punctuation),' ')\n",
        "        string2 = string1.translate(translation_table)\n",
        "        return string2\n",
        "    \n",
        "    X_data_full_clear_punct = []\n",
        "    for i in range(len(X_data_full)):\n",
        "        test_data = remove_punct(X_data_full[i])\n",
        "        X_data_full_clear_punct.append(test_data)\n",
        "        \n",
        "    # function to remove stopwords\n",
        "    def remove_stopwords(X_data_func):\n",
        "        pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
        "        string2 = pattern.sub(' ', X_data_func)\n",
        "        return string2\n",
        "    \n",
        "    X_data_full_clear_stopwords = []\n",
        "    for i in range(len(X_data_full)):\n",
        "        test_data = remove_stopwords(X_data_full[i])\n",
        "        X_data_full_clear_stopwords.append(test_data)\n",
        "        \n",
        "    # function for tokenizing\n",
        "    def tokenize_words(X_data_func):\n",
        "        words = nltk.word_tokenize(X_data_func)\n",
        "        return words\n",
        "    \n",
        "    X_data_full_tokenized_words = []\n",
        "    for i in range(len(X_data_full)):\n",
        "        test_data = tokenize_words(X_data_full[i])\n",
        "        X_data_full_tokenized_words.append(test_data)\n",
        "        \n",
        "    # function for lemmatizing\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    def lemmatize_words(X_data_func):\n",
        "        words = lemmatizer.lemmatize(X_data_func)\n",
        "        return words\n",
        "    \n",
        "    X_data_full_lemmatized_words = []\n",
        "    for i in range(len(X_data_full)):\n",
        "        test_data = lemmatize_words(X_data_full[i])\n",
        "        X_data_full_lemmatized_words.append(test_data)\n",
        "        \n",
        "    # creating the bag of words model\n",
        "    cv = CountVectorizer(max_features=1000)\n",
        "    X_data_full_vector = cv.fit_transform(X_data_full_lemmatized_words).toarray()\n",
        "    \n",
        "    \n",
        "    tfidf = TfidfTransformer()\n",
        "    X_data_full_tfidf = tfidf.fit_transform(X_data_full_vector).toarray()\n",
        "    \n",
        "    return X_data_full_tfidf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C43WklfsL2tI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preparing training and testing data using train_test_split\n",
        "\n",
        "# Data preparation\n",
        "\n",
        "X_data = df['text']\n",
        "y_data = df.type\n",
        "X_data = X_data.astype(str)\n",
        "x_data=final(X_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KurMazGJrt72",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "d18c4a7f-1010-4d1a-d9a5-807f55bbe8fc"
      },
      "source": [
        "# NB fitting and prediction\n",
        "\n",
        "NBX_train, NBX_test, NBy_train, NBy_test = train_test_split(x_data, y_data, test_size=0.3, random_state= seed)\n",
        "\n",
        "MNB = MultinomialNB()\n",
        "MNB.fit(NBX_train, NBy_train)\n",
        "NB_predictions = MNB.predict(NBX_test)\n",
        "\n",
        "# NB Model evaluation\n",
        "\n",
        "print(\"Classification Report\", \"\\n\" ,classification_report(NBy_test, NB_predictions))\n",
        "print(\"Confusion Matrix\", \"\\n\" ,confusion_matrix(NBy_test, NB_predictions))\n",
        "\n",
        "MNB_f1 = round(f1_score(NBy_test, NB_predictions, average='weighted'), 3)\n",
        "MNB_accuracy = round((accuracy_score(NBy_test, NB_predictions)*100),2)\n",
        "\n",
        "print(\"\\n\",\"NB Evaluation:\",\"\\n\",\"Accuracy : \" , MNB_accuracy , \" %\")\n",
        "print(\"f1_score : \" , MNB_f1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.93      0.93      7057\n",
            "           1       0.92      0.92      0.92      6413\n",
            "\n",
            "    accuracy                           0.93     13470\n",
            "   macro avg       0.93      0.93      0.93     13470\n",
            "weighted avg       0.93      0.93      0.93     13470\n",
            "\n",
            "Confusion Matrix \n",
            " [[6568  489]\n",
            " [ 507 5906]]\n",
            "\n",
            " NB Evaluation: \n",
            " Accuracy :  92.61  %\n",
            "f1_score :  0.926\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQwANpN-r7Jw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "7e24e2bb-1f00-49c7-a198-b89ea28f0e68"
      },
      "source": [
        "# RF fitting and prediction\n",
        "\n",
        "RFX_train, RFX_test, RFy_train, RFy_test = train_test_split(x_data, y_data, test_size=0.3, random_state= seed)\n",
        "\n",
        "rfc=RandomForestClassifier(n_estimators= 10, random_state= seed)\n",
        "rfc.fit(RFX_train, RFy_train)\n",
        "RF_predictions = rfc.predict(RFX_test)\n",
        "\n",
        "# RF Model evaluation\n",
        "\n",
        "print(\"Classification Report\", \"\\n\" , classification_report(RFy_test, RF_predictions))\n",
        "print(\"Confusion Matrix\", \"\\n\" ,confusion_matrix(RFy_test, RF_predictions))\n",
        "\n",
        "rfc_f1 = round(f1_score(RFy_test, RF_predictions, average= 'weighted'), 3)\n",
        "rfc_accuracy = round((accuracy_score(RFy_test, RF_predictions) * 100), 2)\n",
        "\n",
        "print(\"\\n\",\"RF Evaluation:\",\"\\n\",\"Accuracy : \" , rfc_accuracy , \" %\")\n",
        "print(\"f1_score : \" , rfc_f1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      7057\n",
            "           1       1.00      0.99      0.99      6413\n",
            "\n",
            "    accuracy                           0.99     13470\n",
            "   macro avg       0.99      0.99      0.99     13470\n",
            "weighted avg       0.99      0.99      0.99     13470\n",
            "\n",
            "Confusion Matrix \n",
            " [[7027   30]\n",
            " [  75 6338]]\n",
            "\n",
            " RF Evaluation: \n",
            " Accuracy :  99.22  %\n",
            "f1_score :  0.992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsP3AgxQ6OKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to predict the type based on input test text\n",
        "\n",
        "def test_news(X_data_full):\n",
        "    \n",
        "    # function for removing punctuations\n",
        "    def remove_punct(X_data_func):\n",
        "        string1 = X_data_func.lower()\n",
        "        translation_table = dict.fromkeys(map(ord, string.punctuation),' ')\n",
        "        string2 = string1.translate(translation_table)\n",
        "        return string2\n",
        "    \n",
        "    X_data_full_clear_punct = []\n",
        "    for i in range(len(X_data_full)):\n",
        "        test_data = remove_punct(X_data_full[i])\n",
        "        X_data_full_clear_punct.append(test_data)\n",
        "        \n",
        "    # function to remove stopwords\n",
        "    def remove_stopwords(X_data_func):\n",
        "        pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
        "        string2 = pattern.sub(' ', X_data_func)\n",
        "        return string2\n",
        "    \n",
        "    X_data_full_clear_stopwords = []\n",
        "    for i in range(len(X_data_full)):\n",
        "        test_data = remove_stopwords(X_data_full[i])\n",
        "        X_data_full_clear_stopwords.append(test_data)\n",
        "        \n",
        "    # function for tokenizing\n",
        "    def tokenize_words(X_data_func):\n",
        "        words = nltk.word_tokenize(X_data_func)\n",
        "        return words\n",
        "    \n",
        "    X_data_full_tokenized_words = []\n",
        "    for i in range(len(X_data_full)):\n",
        "        test_data = tokenize_words(X_data_full[i])\n",
        "        X_data_full_tokenized_words.append(test_data)\n",
        "        \n",
        "    # function for lemmatizing\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    def lemmatize_words(X_data_func):\n",
        "        words = lemmatizer.lemmatize(X_data_func)\n",
        "        return words\n",
        "    \n",
        "    X_data_full_lemmatized_words = []\n",
        "    for i in range(len(X_data_full)):\n",
        "        test_data = lemmatize_words(X_data_full[i])\n",
        "        X_data_full_lemmatized_words.append(test_data)\n",
        "    \n",
        "    # creating the bag of words model\n",
        "    cv = CountVectorizer(max_features=1000)\n",
        "    X_data_full_vector = cv.fit_transform(X_data_full_lemmatized_words).toarray()\n",
        "    \n",
        "    tfidf = TfidfTransformer()\n",
        "    X_data_full_tfidf = tfidf.fit_transform(X_data_full_vector).toarray()\n",
        "    \n",
        "    return X_data_full_tfidf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbYbQCUg3InM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "6b49ed16-7044-44de-d7b2-d401df157d0e"
      },
      "source": [
        "h=[\"break michael flynn crack testifi mueller trump himselfmichael flynn trump embattl former nation secur advi reportedli cave testifi robert mueller team trump collu russia accord abc news special report flynn plead guilti charg includ make fal statement fbi importantli admit plea offici trump transit team direct contact russian officialsfurthermor accord cnn david wright twitter report brian ross report abc news said flynn also say prepar testifi trump order direct make contact russian contradict donald trump said point brianross report michael flynn prepar testifi presid trump candid donald trump order direct make contact russian contradict donald trump said point david wright davidwrightcnn decemb brianross well told flynn made deci cooper last hour distraught deci feel right thing countri david wright davidwrightcnn decemb brianross face huge legal bill million dollar said final go reason expect put hou market face seriou financ problem david wright davidwrightcnn decemb part flynn issu statement say follow action acknowledg court today wrong faith god work set thing right guilti plea agreement cooper special counsel offic reflect deci made best interest famili countri accept full respon action white hou said mere got flynn fire first place zero effect trump har de har har make us laugh hard hurt merri christma donald trump entir treason famili administr hope like orang jumpsuit featur imag via chip somodevillagetti imag\"]\n",
        "\n",
        "h_data=test_news(h)\n",
        "\n",
        "rfc.predict(h_data)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-933285cad9c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mh_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_news\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \"\"\"\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    389\u001b[0m                              \u001b[0;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                              \u001b[0;34m\"input n_features is %s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 1000 and input n_features is 138 "
          ]
        }
      ]
    }
  ]
}